{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "896a905f-5239-4afa-ab38-8bf8778f7d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grey\n"
     ]
    }
   ],
   "source": [
    "print(\"grey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9fa349f-0c59-46a6-96df-bfdc6d5d964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "from self_py_fun.HW10Fun import *\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb6cab9a-c42e-465f-b8cf-2709d5f78a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'Code', 'IndexBegin', 'IndexTag', 'LetterTable', 'Signal', 'Text', 'Type'])\n",
      "(3420, 400)\n",
      "(3420, 1)\n",
      "(1296, 400)\n",
      "(1296, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In HW7, you have the chance to visualize a truncated EEG dataset stratified by\n",
    "# target and non-target stimulus type.\n",
    "#\n",
    "# The fundamental problem of P300 ERP-BCI speller system is to perform a binary classification.\n",
    "#\n",
    "# In HW10, you are asked to implement the binary classification using various methods,\n",
    "# and evaluate the model performance with a testing dataset.\n",
    "#\n",
    "# You will use K114_001_BCI_TRN_Truncated_Data_0.5_6.mat as a training set, and\n",
    "# K114_001_BCI_FRT_Truncated_Data_0.5_6.mat as a testing set.\n",
    "#\n",
    "# Notice that here, we do not split training/testing within K114_001_BCI_TRN_Truncated_Data_0.5_6.mat\n",
    "# because each row is not entirely independent of each other due to the special structure of the dataset.\n",
    "\n",
    "# Global constants:\n",
    "np.random.seed(100)\n",
    "bp_low = 0.5\n",
    "bp_upp = 6\n",
    "electrode_num = 16\n",
    "# Change the following directory to your own one.\n",
    "parent_dir = os.getcwd()\n",
    "parent_data_dir = '{}/data'.format(parent_dir)\n",
    "time_index = np.linspace(0, 800, 25)\n",
    "electrode_name_ls = ['F3', 'Fz', 'F4', 'T7', 'C3', 'Cz', 'C4', 'T8', 'CP3', 'CP4', 'P3', 'Pz', 'P4', 'PO7', 'PO8', 'Oz']\n",
    "subject_name = 'K114'\n",
    "# create a new folder called K114\n",
    "subject_dir = '{}/{}'.format(parent_dir, subject_name)\n",
    "if not os.path.exists(subject_dir):\n",
    "    os.mkdir(subject_dir)\n",
    "\n",
    "char_trn = 'THE0QUICK0BROWN0FOX'\n",
    "char_trn_size = len(char_trn)\n",
    "\n",
    "# Step 1: Import dataset\n",
    "# Step 1.1: TRN dataset\n",
    "trn_data_name = '{}_001_BCI_TRN_Truncated_Data_{}_{}'.format(subject_name, bp_low, bp_upp)\n",
    "trn_data_dir = '{}/{}.mat'.format(parent_data_dir, trn_data_name)\n",
    "eeg_trn_obj = sio.loadmat(trn_data_dir)\n",
    "\n",
    "# eeg_trn_obj is a dictionary!\n",
    "print(eeg_trn_obj.keys())\n",
    "eeg_trn_signal = eeg_trn_obj['Signal']\n",
    "print(eeg_trn_signal.shape) # 3420, 400\n",
    "eeg_trn_type = eeg_trn_obj['Type']\n",
    "print(eeg_trn_type.shape) # 3420, 1\n",
    "eeg_trn_type = np.squeeze(eeg_trn_type, axis=1)\n",
    "\n",
    "# Step 1.2: FRT dataset\n",
    "# The following code should be completed by students themselves.\n",
    "# you should be able to obtain relevant data files named\n",
    "# eeg_frt_signal and eeg_frt_type\n",
    "# Write your own code below:\n",
    "\n",
    "frt_data_name = f\"{subject_name}_001_BCI_FRT_Truncated_Data_{bp_low}_{bp_upp}\"\n",
    "frt_data_dir = f\"{parent_data_dir}/{frt_data_name}.mat\"\n",
    "eeg_frt_obj = sio.loadmat(frt_data_dir)\n",
    "\n",
    "eeg_frt_signal = eeg_frt_obj[\"Signal\"]\n",
    "print(eeg_frt_signal.shape)\n",
    "\n",
    "eeg_frt_type = eeg_frt_obj[\"Type\"]\n",
    "print(eeg_frt_type.shape)\n",
    "eeg_frt_type = eeg_frt_type.ravel()\n",
    "\n",
    "\n",
    "# You have completed the exploratory data analysis in HW7 and HW8.\n",
    "# The dataset has been carefully reviewed by Dr. Jane E. Huggins,\n",
    "# so we do not need to worry about missing, outliers, errors of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e862eda-7b6e-4661-9b9f-579f6c63949b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression on TRN:\n",
      "[['Z' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q']\n",
      " ['U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U']\n",
      " ['C' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I']\n",
      " ['C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C']\n",
      " ['K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']\n",
      " ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n",
      " ['4' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']]\n",
      "['T', 'H', 'E', '0', 'Q', 'U', 'I', 'C', 'K', '0', 'B', 'R', 'O', 'W', 'N', '0', 'F', 'O', 'X']\n",
      "Logistic Regression on FRT:\n",
      "[['H' 'H' 'H' 'T']\n",
      " ['8' '8' '8' '8']\n",
      " ['N' 'T' 'N' 'T']\n",
      " ['H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0']\n",
      " ['2' '8' '8' '8']\n",
      " ['0' '0' '0' '0']\n",
      " ['P' 'D' 'D' 'D']\n",
      " ['O' 'O' 'O' 'O']\n",
      " ['G' 'G' 'G' 'G']\n",
      " ['0' '0' '0' '0']\n",
      " ['B' 'B' 'B' 'B']\n",
      " ['U' 'U' 'U' 'U']\n",
      " ['0' '0' 'R' 'R']\n",
      " ['I' 'I' 'I' 'I']\n",
      " ['K' 'E' 'E' 'E']\n",
      " ['F' 'D' 'D' 'D']\n",
      " ['0' '0' '0' '0']\n",
      " ['N' 'T' 'T' 'T']\n",
      " ['H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0']\n",
      " ['T' 'B' 'B' 'B']\n",
      " ['O' 'O' 'O' 'O']\n",
      " ['N' 'N' 'N' 'N']\n",
      " ['E' 'E' 'E' 'E']]\n",
      "['T', '8', 'T', 'H', 'E', '0', '8', '0', 'D', 'O', 'G', '0', 'B', 'U', 'R', 'I', 'E', 'D', '0', 'T', 'H', 'E', '0', 'B', 'O', 'N', 'E']\n",
      "LDA on TRN:\n",
      "[['Z' 'Z' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q']\n",
      " ['U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U']\n",
      " ['C' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I']\n",
      " ['C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C']\n",
      " ['K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']\n",
      " ['H' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n",
      " ['4' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']]\n",
      "['T', 'H', 'E', '0', 'Q', 'U', 'I', 'C', 'K', '0', 'B', 'R', 'O', 'W', 'N', '0', 'F', 'O', 'X']\n",
      "LDA on FRT:\n",
      "[['H' 'H' 'H' 'H']\n",
      " ['8' '8' '8' '8']\n",
      " ['N' 'N' 'N' 'T']\n",
      " ['H' 'B' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0']\n",
      " ['2' '8' '8' '8']\n",
      " ['0' '0' '0' '0']\n",
      " ['D' 'D' 'D' 'D']\n",
      " ['O' 'O' 'O' 'O']\n",
      " ['G' 'G' 'G' 'G']\n",
      " ['4' '0' '0' '0']\n",
      " ['B' 'B' 'B' 'B']\n",
      " ['U' 'U' 'U' 'U']\n",
      " ['4' '4' 'R' 'R']\n",
      " ['I' 'I' 'I' 'I']\n",
      " ['E' 'E' 'E' 'E']\n",
      " ['F' 'D' 'D' 'D']\n",
      " ['0' '0' '0' '0']\n",
      " ['N' 'T' 'T' 'T']\n",
      " ['H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0']\n",
      " ['T' 'B' 'B' 'B']\n",
      " ['O' 'O' 'O' 'O']\n",
      " ['N' 'N' 'N' 'N']\n",
      " ['E' 'E' 'E' 'E']]\n",
      "['T', '8', 'T', 'H', 'E', '0', '8', '0', 'D', 'O', 'G', '0', 'B', 'U', 'R', 'I', 'E', 'D', '0', 'T', 'H', 'E', '0', 'B', 'O', 'N', 'E']\n",
      "Support Vector Machine on TRN:\n",
      "[['Z' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T' 'T']\n",
      " ['H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q' 'Q']\n",
      " ['U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U' 'U']\n",
      " ['I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I' 'I']\n",
      " ['C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C' 'C']\n",
      " ['K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K' 'K']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B' 'B']\n",
      " ['R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'R']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W' 'W']\n",
      " ['N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N' 'N']\n",
      " ['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0']\n",
      " ['F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F' 'F']\n",
      " ['O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O' 'O']\n",
      " ['X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X' 'X']]\n",
      "['T', 'H', 'E', '0', 'Q', 'U', 'I', 'C', 'K', '0', 'B', 'R', 'O', 'W', 'N', '0', 'F', 'O', 'X']\n",
      "Support Vector Machine on FRT:\n",
      "[['H' 'H' 'H' 'H']\n",
      " ['8' '8' '8' '8']\n",
      " ['N' 'T' 'N' 'T']\n",
      " ['H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0']\n",
      " ['Z' '8' '8' '8']\n",
      " ['0' '0' '0' '0']\n",
      " ['P' 'D' 'D' 'D']\n",
      " ['O' 'O' 'O' 'O']\n",
      " ['G' 'G' 'G' 'G']\n",
      " ['4' '0' '0' '0']\n",
      " ['B' 'B' 'B' 'B']\n",
      " ['U' 'U' 'U' 'U']\n",
      " ['4' '4' 'R' 'R']\n",
      " ['I' 'I' 'I' 'I']\n",
      " ['K' 'E' 'E' 'E']\n",
      " ['F' 'D' 'D' 'D']\n",
      " ['0' '0' '0' '0']\n",
      " ['N' 'T' 'T' 'T']\n",
      " ['H' 'H' 'H' 'H']\n",
      " ['E' 'E' 'E' 'E']\n",
      " ['0' '0' '0' '0']\n",
      " ['T' 'B' 'B' 'B']\n",
      " ['O' 'O' 'O' 'O']\n",
      " ['N' 'N' 'N' 'N']\n",
      " ['E' 'E' 'E' 'E']]\n",
      "['T', '8', 'T', 'H', 'E', '0', '8', '0', 'D', 'O', 'G', '0', 'B', 'U', 'R', 'I', 'E', 'D', '0', 'T', 'H', 'E', '0', 'B', 'O', 'N', 'E']\n",
      "[0.84210526 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.        ]\n",
      "[0.78947368 0.94736842 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.        ]\n",
      "[0.94736842 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.        ]\n",
      "[0.66666667 0.92592593 0.92592593 1.        ]\n",
      "[0.7037037  0.85185185 0.92592593 0.96296296]\n",
      "[0.62962963 0.92592593 0.92592593 0.96296296]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Fit classification models\n",
    "# You will try the following methods:\n",
    "# Logistic Regression,\n",
    "# Linear Discriminant Analysis,\n",
    "# Support Vector Machine (sometimes called support vector classification)\n",
    "# You do not need to modify the parameters of each classifier\n",
    "# except for LogisticRegression: set max_iter=1000\n",
    "# Write your own code below:\n",
    "scaler = StandardScaler()\n",
    "eeg_trn_signal = scaler.fit_transform(eeg_trn_signal)\n",
    "eeg_frt_signal = scaler.transform(eeg_frt_signal)\n",
    "\n",
    "log_reg = LR(max_iter=1000, random_state=100)\n",
    "log_reg.fit(eeg_trn_signal, eeg_trn_type)\n",
    "\n",
    "lda = LDA()\n",
    "lda.fit(eeg_trn_signal, eeg_trn_type)\n",
    "\n",
    "\n",
    "svc_prob = SVC(probability=True, kernel='linear', random_state=100)\n",
    "svc_prob.fit(eeg_trn_signal, eeg_trn_type)\n",
    "\n",
    "svc = SVC(random_state=100)\n",
    "svc.fit(eeg_trn_signal, eeg_trn_type)\n",
    "\n",
    "# Step 3: Evaluate model performance on both TRN and FRT files\n",
    "# Step 3.1: Prediction accuracy on TRN files\n",
    "# We will compute the probability of each stimulus with .predict_proba()\n",
    "# before we convert the stimulus-level probability to character-level probability.\n",
    "# You are asked to generate stimulus-level probability for each method on TRN files,\n",
    "# denoted as logistic_y_trn, lda_y_trn, and svm_y_trn.\n",
    "# Write your own code below:\n",
    "\n",
    "logistic_y_trn = log_reg.predict_proba(eeg_trn_signal)\n",
    "lda_y_trn = lda.predict_proba(eeg_trn_signal)\n",
    "svm_y_trn = svc_prob.predict_proba(eeg_trn_signal)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3.2: Prediction accuracy on FRT files\n",
    "# Similarly, you are asked to generate stimulus-level probability for each method on FRT files,\n",
    "# denoted as logistic_y_frt, lda_y_frt, and svm_y_frt.\n",
    "# Write your own code below:\n",
    "\n",
    "logistic_y_frt = log_reg.predict_proba(eeg_frt_signal)\n",
    "lda_y_frt = lda.predict_proba(eeg_frt_signal)\n",
    "svm_y_frt = svc_prob.predict_proba(eeg_frt_signal)\n",
    "\n",
    "# Step 4: Convert binary classification probability to character-level accuracy\n",
    "# This involves advanced data manipulation, so you do not need to write any new code.\n",
    "# Please run the following code to view the final results.\n",
    "\n",
    "eeg_trn_code = eeg_trn_obj['Code']\n",
    "eeg_frt_code = eeg_frt_obj['Code']\n",
    "char_frt = convert_raw_char_to_alphanumeric_stype(eeg_frt_obj['Text'])\n",
    "\n",
    "# raw format is different from the current 6x6 layout characters.\n",
    "char_frt_size = len(char_frt)\n",
    "frt_seq_size = int(eeg_frt_signal.shape[0]/char_frt_size/12)\n",
    "\n",
    "# Logistic regression\n",
    "print('Logistic Regression on TRN:')\n",
    "logistic_letter_mat_trn, logistic_letter_prob_mat_trn = streamline_predict(\n",
    "    logistic_y_trn, eeg_trn_type, eeg_trn_code, char_trn_size, trn_seq_size,\n",
    "    stimulus_group_set, eeg_rcp_array\n",
    ")\n",
    "print(logistic_letter_mat_trn)\n",
    "print(list(char_trn)) # This is the true spelling characters for training set!\n",
    "logistic_trn_accuracy = np.mean(logistic_letter_mat_trn == np.array(list(char_trn))[:, np.newaxis], axis=0)\n",
    "\n",
    "print('Logistic Regression on FRT:')\n",
    "logistic_letter_mat_frt, logistic_letter_prob_mat_frt = streamline_predict(\n",
    "    logistic_y_frt, eeg_frt_type, eeg_frt_code, char_frt_size, frt_seq_size,\n",
    "    stimulus_group_set, eeg_rcp_array\n",
    ")\n",
    "print(logistic_letter_mat_frt)\n",
    "print(list(char_frt)) # This is the true spelling characters for testing set!\n",
    "logistic_frt_accuracy = np.mean(logistic_letter_mat_frt == np.array(list(char_frt))[:, np.newaxis], axis=0)\n",
    "\n",
    "# LDA:\n",
    "print('LDA on TRN:')\n",
    "lda_letter_mat_trn, lda_letter_prob_mat_trn = streamline_predict(\n",
    "    lda_y_trn, eeg_trn_type, eeg_trn_code, char_trn_size, trn_seq_size,\n",
    "    stimulus_group_set, eeg_rcp_array\n",
    ")\n",
    "print(lda_letter_mat_trn)\n",
    "print(list(char_trn)) # This is the true spelling characters for training set!\n",
    "lda_trn_accuracy = np.mean(lda_letter_mat_trn == np.array(list(char_trn))[:, np.newaxis], axis=0)\n",
    "\n",
    "print('LDA on FRT:')\n",
    "lda_letter_mat_frt, lda_letter_prob_mat_frt = streamline_predict(\n",
    "    lda_y_frt, eeg_frt_type, eeg_frt_code, char_frt_size, frt_seq_size,\n",
    "    stimulus_group_set, eeg_rcp_array\n",
    ")\n",
    "print(lda_letter_mat_frt)\n",
    "print(list(char_frt)) # This is the true spelling characters for testing set!\n",
    "lda_frt_accuracy = np.mean(lda_letter_mat_frt == np.array(list(char_frt))[:, np.newaxis], axis=0)\n",
    "\n",
    "# SVM:\n",
    "print('Support Vector Machine on TRN:')\n",
    "svm_letter_mat_trn, svm_letter_prob_mat_trn = streamline_predict(\n",
    "    svm_y_trn, eeg_trn_type, eeg_trn_code, char_trn_size, trn_seq_size,\n",
    "    stimulus_group_set, eeg_rcp_array\n",
    ")\n",
    "print(svm_letter_mat_trn)\n",
    "print(list(char_trn)) # This is the true spelling characters for training set!\n",
    "svm_trn_accuracy = np.mean(svm_letter_mat_trn == np.array(list(char_trn))[:, np.newaxis], axis=0)\n",
    "\n",
    "print('Support Vector Machine on FRT:')\n",
    "svm_letter_mat_frt, svm_letter_prob_mat_frt = streamline_predict(\n",
    "    svm_y_frt, eeg_frt_type, eeg_frt_code, char_frt_size, frt_seq_size,\n",
    "    stimulus_group_set, eeg_rcp_array\n",
    ")\n",
    "print(svm_letter_mat_frt)\n",
    "print(list(char_frt)) # This is the true spelling characters for training set!\n",
    "svm_frt_accuracy = np.mean(svm_letter_mat_frt == np.array(list(char_frt))[:, np.newaxis], axis=0)\n",
    "\n",
    "\n",
    "print(logistic_trn_accuracy)\n",
    "print(lda_trn_accuracy)\n",
    "print(svm_trn_accuracy)\n",
    "\n",
    "print(logistic_frt_accuracy)\n",
    "print(lda_frt_accuracy)\n",
    "print(svm_frt_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8282d3d-b76f-469a-aab4-c2660a97d28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trn:\n",
      "[0.84210526 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.        ]\n",
      "[0.78947368 0.94736842 1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.        ]\n",
      "[0.94736842 1.         1.         1.         1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 1.         1.         1.        ]\n",
      "frt:\n",
      "[0.66666667 0.92592593 0.92592593 1.        ]\n",
      "[0.7037037  0.85185185 0.92592593 0.96296296]\n",
      "[0.62962963 0.92592593 0.92592593 0.96296296]\n"
     ]
    }
   ],
   "source": [
    "print(\"trn:\")\n",
    "print(logistic_trn_accuracy)\n",
    "print(lda_trn_accuracy)\n",
    "print(svm_trn_accuracy)\n",
    "\n",
    "print(\"frt:\")\n",
    "print(logistic_frt_accuracy)\n",
    "print(lda_frt_accuracy)\n",
    "print(svm_frt_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a871a370-a1e9-4df4-8d49-5b6183455419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to answer two questions below:\n",
    "\n",
    "# What do rows 122, 131, 141, 150, 160, and 169 do? Briefly answer the question below:\n",
    "# In case that your row IDs are messed up when you start to fill in the blank,\n",
    "# I attach the lines of code for your reference.\n",
    "# logistic_trn_accuracy = np.mean(logistic_letter_mat_trn == np.array(list(char_trn))[:, np.newaxis], axis=0)\n",
    "# logistic_frt_accuracy = np.mean(logistic_letter_mat_frt == np.array(list(char_frt))[:, np.newaxis], axis=0)\n",
    "# lda_trn_accuracy = np.mean(lda_letter_mat_trn == np.array(list(char_trn))[:, np.newaxis], axis=0)\n",
    "# lda_frt_accuracy = np.mean(lda_letter_mat_frt == np.array(list(char_frt))[:, np.newaxis], axis=0)\n",
    "# svm_trn_accuracy = np.mean(svm_letter_mat_trn == np.array(list(char_trn))[:, np.newaxis], axis=0)\n",
    "# svm_frt_accuracy = np.mean(svm_letter_mat_frt == np.array(list(char_frt))[:, np.newaxis], axis=0)\n",
    "\n",
    "#Each line computes the character-level accuracy for one classifier. \n",
    "\n",
    "# Step 5: Summary\n",
    "# Which method performs the best? Why?\n",
    "#LDA, this process is more effectivefor computing and efficient at classification of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f89e2ee-34d1-42b2-8471-be564a45cb49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
